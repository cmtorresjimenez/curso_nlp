{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_head_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ePC3ww9GvZ4j"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmtorresjimenez/curso_nlp/blob/main/3_arquitectura_transformer/multi_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM565xt5Cs23"
      },
      "source": [
        "# Módulo de Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJa5Qpjcs_UY"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "import math\n",
        "from typing import Optional"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKeAC6mzCz98"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-z2WhDjfsJT"
      },
      "source": [
        "Una función de atención puede ser decrita como un mapeo de una consulta (query) y un conjunto de parejas llave-valor (key-value) a una salida, donde consultas, llaves, valores y salidas son todos vectores. La salida se calcula como una suma ponderada de los valores, donde el peso asignado a cada uno de los valores es calculado por una función de compatibilidad entre cada consulta y la correspodiente llave.\n",
        "\n",
        "En Transformers, dicha función atención se denomina \"Scaled Dot-Product Attention\". La entrada consiste en consultas y llaves de dimensión $d_k$, y valores de dimensión $d_v$. Calculamos el producto punto de la consulta con todas las llave, divimos cada producto por $\\sqrt{d_k}$, y aplicamos una función softmax para obtener los pesos sobre los valores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUFyjkTFpAJf"
      },
      "source": [
        "En la práctica, calculamos la función de atención sobre un conjunto de consultas de manera simultanea, acopladas en una matriz $Q$. Las llaves y valores también se acoplan en matricez $K$ y $V$ respectivamente. Calculamos la matriz de salidas de la siguiente manera:\n",
        "\n",
        "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9h7xzFxqOPv"
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    '''\n",
        "    BLoque de MultiHeadedAttention que permita al modelo atender de manera\n",
        "    conjunta a información de diferentes subespacios de representación.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): número de cabezas por capa\n",
        "        d_model (int): dimensión total del modelo\n",
        "        dropout (float): Una capa de dropout sobre attention_probs. Default: 0.0. \n",
        "    '''\n",
        "    def __init__(self, num_heads: int, d_model: int, dropout: float = 0.0):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        if d_model % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({d_model}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({num_heads})\"\n",
        "            )\n",
        "        # Número de features por cabeza, se asume que d_v = d_k\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_heads, self.d_k)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: Tensor,\n",
        "        key: Tensor,\n",
        "        value: Tensor,\n",
        "        mask: Optional[Tensor] = None,\n",
        "        output_attentions: Optional[bool] = False\n",
        "    ):\n",
        "        '''\n",
        "        Args:\n",
        "            query, key, value: Se mapea el query y un conjunto de parejas key-value a una salida output.\n",
        "            mask: máscara que previene la atención en ciertas posiciones.\n",
        "            output_attentions: Indica si se quiere regresar la matriz de pesos de atención\n",
        "        '''\n",
        "        if mask is not None:\n",
        "            # Se aplica la misma máscara para todas las cabezas\n",
        "            mask = mask.unsqueeze(1)\n",
        "        \n",
        "        query_layer = self.transpose_for_scores(self.query(query)) # (batch, num_heads, seq_len, d_k)\n",
        "        key_layer = self.transpose_for_scores(self.key(key))\n",
        "        value_layer = self.transpose_for_scores(self.value(value))\n",
        "\n",
        "        # Se realiza el producto punto entre \"query\" y \"key\" para obtener los scores de atención crudos/sin procesar\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.d_k)\n",
        "\n",
        "        # Se aplica máscara\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Se normalizan los scores de atención a probabilidades\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous() # (batch, seq_len, num_heads, d_k)\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.d_model,) # (batch, seq_len, d_model)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    '''\n",
        "    BLoque de MultiHeadedAttention que permita al modelo atender de manera\n",
        "    conjunta a información de diferentes subespacios de representación.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): número de cabezas por capa\n",
        "        d_model (int): dimensión total del modelo\n",
        "        dropout (float): Una capa de dropout sobre attention_probs. Default: 0.0. \n",
        "    '''\n",
        "    def __init__(self, num_heads: int, d_model: int, dropout: float = 0.0):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        if d_model % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({d_model}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({num_heads})\"\n",
        "            )\n",
        "        # Número de features por cabeza, se asume que d_v = d_k\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.norm = nn.BatchNorm1d(d_model)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: Tensor,\n",
        "        key: Tensor,\n",
        "        value: Tensor,\n",
        "        mask: Optional[Tensor] = None,\n",
        "        output_attentions: Optional[bool] = False\n",
        "    ):\n",
        "        '''\n",
        "        Args:\n",
        "            query, key, value: Se mapea el query y un conjunto de parejas key-value a una salida output.\n",
        "            mask: máscara que previene la atención en ciertas posiciones.\n",
        "            output_attentions: Indica si se quiere regresar la matriz de pesos de atención\n",
        "        '''\n",
        "        key_layer = self.key(key)\n",
        "        value_layer = self.value(value)\n",
        "\n",
        "        # Se realiza el producto punto entre \"query\" y \"key\" para obtener los scores de atención crudos/sin procesar\n",
        "        attention_scores = torch.matmul(query, key_layer.transpose(-2, -1))\n",
        "        attention_scores = attention_scores / math.sqrt(self.d_k)\n",
        "\n",
        "        # Se normalizan los scores de atención a probabilidades\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        #attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer) + query\n",
        "\n",
        "        context_layer = self.norm(context_layer.transpose(-1, -2)).transpose(-1, -2)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "CGGRjudqtajA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrexUyO75cif"
      },
      "source": [
        "## Ejemplo uso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbLMgqVg53Po"
      },
      "source": [
        "Instanciamos el módulo de Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_1uE963ynLz"
      },
      "source": [
        "att = MultiHeadedAttention(8, 768, 0.1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPt0X84-J7f7"
      },
      "source": [
        "Creamos un entrada con tamaño de lote de 8 y secuencias de 300 elementos aleatorios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGI9q_b_zQSs"
      },
      "source": [
        "# Se prepara la entrada\n",
        "x = torch.rand(8, 300, 768)\n",
        "mask = torch.ones((8, 300))\n",
        "\n",
        "# Ejecutar módulo, se regresa la matriz de atención\n",
        "output = att(query=x, key=x, value=x, mask=mask, output_attentions=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422xa6Wi5wj4"
      },
      "source": [
        "Embeddings de salida"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEENWTgi5J4M",
        "outputId": "24960108-e711-444f-d366-99d34b3143c3"
      },
      "source": [
        "print(output[0].shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 300, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QVK5OT95znL"
      },
      "source": [
        "Matriz de atención"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC-bKXEU5bXw",
        "outputId": "54c1b648-5c40-4095-eda3-2f174d3efcbe"
      },
      "source": [
        "print(output[1].shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 300, 300])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NjANMCg6Nh6"
      },
      "source": [
        "## Visualización de self-attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NUqRP0r6jO9"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install bertviz"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8KdVw4v6RNC"
      },
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "from bertviz import head_view"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "kn3WDSKn6scz",
        "outputId": "de5d2925-9f1b-4d75-e60f-266a158d8804"
      },
      "source": [
        "bert = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
        "bert_embeddings_layer = bert.embeddings\n",
        "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
        "\n",
        "att = MultiHeadedAttention(8, 768, 0.1)\n",
        "\n",
        "att.query.load_state_dict(bert.encoder.layer[0].attention.self.query.state_dict())\n",
        "att.key.load_state_dict(bert.encoder.layer[0].attention.self.key.state_dict())\n",
        "att.value.load_state_dict(bert.encoder.layer[0].attention.self.value.state_dict())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4a3be80021b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadedAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0matt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0matt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0matt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MultiHeadedAttention' object has no attribute 'query'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkf49jHH7FqQ"
      },
      "source": [
        "text_input = tokenizer([\"El perro va caminando sobre el pasto\"], return_tensors='pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9cb-4p37hj1"
      },
      "source": [
        "input_ids = text_input['input_ids']\n",
        "x = bert_embeddings_layer(input_ids)\n",
        "mask = text_input['attention_mask']\n",
        "\n",
        "output, attention_scores = att(query=x, key=x, value=x, mask=mask, output_attentions=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bDFndu18sir"
      },
      "source": [
        "attention_scores.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi-t3hkA-FWX"
      },
      "source": [
        "input_id_list = input_ids.tolist()[0]\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_id_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M45jMHwx-dk9"
      },
      "source": [
        "head_view((attention_scores,)*12, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting Letters"
      ],
      "metadata": {
        "id": "cN9r_Wa_Qn7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string"
      ],
      "metadata": {
        "id": "Ib8BIpNyQx3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    '''\n",
        "    BLoque de MultiHeadedAttention que permita al modelo atender de manera\n",
        "    conjunta a información de diferentes subespacios de representación.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): número de cabezas por capa\n",
        "        d_model (int): dimensión total del modelo\n",
        "        dropout (float): Una capa de dropout sobre attention_probs. Default: 0.0. \n",
        "    '''\n",
        "    def __init__(self, num_heads: int, d_model: int, dropout: float = 0.0):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        if d_model % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({d_model}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({num_heads})\"\n",
        "            )\n",
        "        # Número de features por cabeza, se asume que d_v = d_k\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.norm = nn.BatchNorm1d(d_model)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: Tensor,\n",
        "        key: Tensor,\n",
        "        value: Tensor,\n",
        "        mask: Optional[Tensor] = None,\n",
        "        output_attentions: Optional[bool] = False\n",
        "    ):\n",
        "        '''\n",
        "        Args:\n",
        "            query, key, value: Se mapea el query y un conjunto de parejas key-value a una salida output.\n",
        "            mask: máscara que previene la atención en ciertas posiciones.\n",
        "            output_attentions: Indica si se quiere regresar la matriz de pesos de atención\n",
        "        '''\n",
        "        key_layer = self.key(key)\n",
        "        value_layer = self.value(value)\n",
        "\n",
        "        # Se realiza el producto punto entre \"query\" y \"key\" para obtener los scores de atención crudos/sin procesar\n",
        "        attention_scores = torch.matmul(query, key_layer.transpose(-2, -1))\n",
        "        attention_scores = attention_scores / math.sqrt(self.d_k)\n",
        "\n",
        "        # Se normalizan los scores de atención a probabilidades\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        #attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer) + query\n",
        "\n",
        "        context_layer = self.norm(context_layer.transpose(-1, -2)).transpose(-1, -2)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "4AED2u7rVfC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    values = torch.matmul(attention, v) + q\n",
        "    return values, attention\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.d_k = embed_dim // num_heads\n",
        "        self.d_model = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        #self.dropout = nn.Dropout(p=dropout)\n",
        "        self.norm = nn.BatchNorm1d(embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "    \n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None, return_attention=False):\n",
        "        key_layer = self.key(key)\n",
        "        value_layer = self.value(value)\n",
        "\n",
        "        # Determine value outputs\n",
        "        o, attention = scaled_dot_product(query, key_layer, value_layer, mask=mask)\n",
        "        #o = self.norm(values.transpose(-1, -2)).transpose(-1, -2)\n",
        "        #values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        #values = values.reshape(batch_size, seq_length, embed_dim)\n",
        "        #o = self.o_proj(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o"
      ],
      "metadata": {
        "id": "87uqpIjgLR0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CounterModel(nn.Module):\n",
        "    def __init__(self, num_heads: int = 1, d_model: int = 64, dropout: float = 0.1, emb_dim: int = 64, vocab_size: int = 3, max_len: int = 10):\n",
        "        super(CounterModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embeddings = nn.Embedding(num_embeddings=vocab_size+1, embedding_dim=emb_dim)\n",
        "        self.query = nn.Linear(in_features=emb_dim, out_features=1).weight\n",
        "        self.mh_att = MultiheadAttention(d_model, emb_dim, num_heads)\n",
        "        self.classification_head = nn.Linear(in_features=emb_dim, out_features=max_len+1)\n",
        "    \n",
        "    def forward(self, letter_sequence):\n",
        "        batch_size = input_batch.shape[0]\n",
        "        letter_sequence_ids = (input_batch == 1).nonzero(as_tuple=True)[-1].reshape((batch_size,-1))\n",
        "        x = self.embeddings(letter_sequence_ids)\n",
        "        q_r = self.query.repeat((batch_size,self.vocab_size,1))\n",
        "        output, attention_scores = self.mh_att(query=q_r, key=x, value=x, mask=None, return_attention=True)\n",
        "        logits = self.classification_head(output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "hLC62li5LRK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CounterModel(nn.Module):\n",
        "    def __init__(self, num_heads: int = 1, d_model: int = 64, dropout: float = 0.1, emb_dim: int = 64, vocab_size: int = 3, max_len: int = 10):\n",
        "        super(CounterModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embeddings = nn.Embedding(num_embeddings=vocab_size+1, embedding_dim=emb_dim)\n",
        "        self.query = nn.Linear(in_features=emb_dim, out_features=1).weight\n",
        "        self.mh_att = MultiHeadedAttention(num_heads, d_model, dropout)\n",
        "        self.classification_head = nn.Linear(in_features=emb_dim, out_features=max_len+1)\n",
        "    \n",
        "    def forward(self, letter_sequence):\n",
        "        #print(letter_sequence.shape)\n",
        "        batch_size = input_batch.shape[0]\n",
        "        letter_sequence_ids = (input_batch == 1).nonzero(as_tuple=True)[-1].reshape((batch_size,-1))\n",
        "        #cls_tokens = torch.ones((batch_size,1), dtype=torch.long)*(self.vocab_size+1)\n",
        "        #letter_sequence_ids = torch.concat((cls_tokens, letter_sequence_ids), 1)\n",
        "        x = self.embeddings(letter_sequence_ids)\n",
        "        #print(x.shape)\n",
        "        q_r = self.query.repeat((batch_size,self.vocab_size,1))\n",
        "        output, attention_scores = self.mh_att(query=q_r, key=x, value=x, mask=None, output_attentions=True)\n",
        "        #print(output.shape)\n",
        "        logits = self.classification_head(output)\n",
        "        #print(logits.shape)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "uHsCOAQqRt4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Task(object):\n",
        "\n",
        "\tdef __init__(self, max_len=10, vocab_size=3):\n",
        "\t\tsuper(Task, self).__init__()\n",
        "\t\tself.max_len = max_len\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tassert self.vocab_size <= 26, \"vocab_size needs to be <= 26 since we are using letters to prettify LOL\"\n",
        "\n",
        "\tdef next_batch(self, batchsize=100):\n",
        "\t\tx = np.eye(self.vocab_size + 1)[np.random.choice(np.arange(self.vocab_size + 1), [batchsize, self.max_len])]\n",
        "\t\ty = np.eye(self.max_len + 1)[np.sum(x, axis=1)[:, 1:].astype(np.int32)]\n",
        "\t\treturn x, y\n",
        "\n",
        "\tdef prettify(self, samples):\n",
        "\t\tsamples = samples.reshape(-1, self.max_len, self.vocab_size + 1)\n",
        "\t\tidx = np.expand_dims(np.argmax(samples, axis=2), axis=2)\n",
        "\t\tdictionary = np.array(list(' ' + string.ascii_uppercase))\n",
        "\t\treturn dictionary[idx]"
      ],
      "metadata": {
        "id": "K4VVUEq-QrtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = Task(max_len=10, vocab_size=3)"
      ],
      "metadata": {
        "id": "VSz6GEu2Q4XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minibatch_x, minibatch_y = task.next_batch(batchsize=2)"
      ],
      "metadata": {
        "id": "2caDtmD_RBpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(minibatch_x)"
      ],
      "metadata": {
        "id": "bcjH5LGFRHG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(minibatch_y)"
      ],
      "metadata": {
        "id": "3PWZGEgjRIAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minibatch_y.shape"
      ],
      "metadata": {
        "id": "Fp36kLQFmMrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task.prettify(minibatch_x)"
      ],
      "metadata": {
        "id": "ExQE_4zKRYIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch = torch.tensor(minibatch_x, dtype=torch.long)"
      ],
      "metadata": {
        "id": "0vB_Mu56VFY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch_ids = (input_batch == 1).nonzero(as_tuple=True)[-1].reshape((input_batch.shape[0],-1))"
      ],
      "metadata": {
        "id": "WDN_pobbWVcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_token = torch.ones((2,1), dtype=torch.long)*4"
      ],
      "metadata": {
        "id": "rvtKPox_ds5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.concat((cls_token, input_batch_ids), 1)"
      ],
      "metadata": {
        "id": "7sUXYm31eG2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CounterModel()"
      ],
      "metadata": {
        "id": "3Bv_JT9fVP6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(input_batch).shape"
      ],
      "metadata": {
        "id": "wWcCRxlbVroY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = nn.Linear(in_features=10, out_features=1, bias=False).weight"
      ],
      "metadata": {
        "id": "46wrcGN4jmi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.shape"
      ],
      "metadata": {
        "id": "9_8-X7bej7DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_r = q.repeat((2,3,1))"
      ],
      "metadata": {
        "id": "y-T57P5Xkpf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_r.shape"
      ],
      "metadata": {
        "id": "7703EgfIj9gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.rand((2,5,10))"
      ],
      "metadata": {
        "id": "lFnQLEcQkPod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = torch.rand((2,5,10))"
      ],
      "metadata": {
        "id": "6d1AOWOvkw7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = torch.matmul(q_r, k.transpose(-1, -2))"
      ],
      "metadata": {
        "id": "98ailTG1kIa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs.shape"
      ],
      "metadata": {
        "id": "CpJ7AX6MlIqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_layer = torch.matmul(probs, v)"
      ],
      "metadata": {
        "id": "aH9v-e2bk6ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_layer.shape"
      ],
      "metadata": {
        "id": "BDCma6XblJ9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "QFSBMMLbmP1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CounterModel(num_heads=1, d_model=64, dropout=0.1, emb_dim=64)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "my9WUqITmQ85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()"
      ],
      "metadata": {
        "id": "p8MQLEyWsjFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.embeddings.weight.shape"
      ],
      "metadata": {
        "id": "bBDIOErU0v0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "-nmMS7re0XHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = Task(max_len=10, vocab_size=3)\n",
        "dataset = [task.next_batch(batchsize=128) for i in range(100)]"
      ],
      "metadata": {
        "id": "Ra9Z63ewzaWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(5000):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    i = step%100\n",
        "    #input_batch, label_batch = task.next_batch(batchsize=128)\n",
        "    input_batch, label_batch = dataset[0]\n",
        "    input_batch = torch.tensor(input_batch, dtype=torch.long)\n",
        "    label_batch = torch.tensor(label_batch, dtype=torch.float)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(input_batch)\n",
        "    loss = F.cross_entropy(outputs, label_batch)\n",
        "    loss.backward()\n",
        "    for name, param in model.named_parameters():\n",
        "        #if \"classification_head\" in name:\n",
        "        #print(param.grad.shape)\n",
        "        if param.grad is None:\n",
        "            print(f\"NONE: {name}\")\n",
        "            break\n",
        "        else:\n",
        "            print(name, param.grad)\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    if step % 100 == 0:\n",
        "        print(f'step: {step + 1} | loss: {loss.item():.3f}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "on3gJEP1nPTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF training"
      ],
      "metadata": {
        "id": "ePC3ww9GvZ4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==1.10"
      ],
      "metadata": {
        "id": "OnNUgc1Fx9Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/greentfrapp/attention-primer.git"
      ],
      "metadata": {
        "id": "fjSUFUJFvbmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python attention-primer/1_counting-letters/main.py --train"
      ],
      "metadata": {
        "id": "3j2mR6s4vi91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}